{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "# The batch size, number of training epochs and location of the data files is defined here. \n",
        "# Data files are hosted in a Google Cloud Storage (GCS) bucket which is why their address starts with gs://.\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "training_images_file   = 'gs://mnist-public/train-images-idx3-ubyte'\n",
        "training_labels_file   = 'gs://mnist-public/train-labels-idx1-ubyte'\n",
        "validation_images_file = 'gs://mnist-public/t10k-images-idx3-ubyte'\n",
        "validation_labels_file = 'gs://mnist-public/t10k-labels-idx1-ubyte'"
      ],
      "metadata": {
        "id": "B15xRwT8ceKN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "# All the necessary Python libraries are imported here, including TensorFlow.\n",
        "\n",
        "import os, re, math, json, shutil, pprint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVk5axPPel2o",
        "outputId": "701e3144-5a03-4176-95d7-f57b8de25fbd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version 2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse files and prepare training and validation datasets.\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def read_label(tf_bytestring):\n",
        "    label = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    label = tf.reshape(label, [])\n",
        "    label = tf.one_hot(label, 10)\n",
        "    return label\n",
        "  \n",
        "def read_image(tf_bytestring):\n",
        "    image = tf.io.decode_raw(tf_bytestring, tf.uint8)\n",
        "    image = tf.cast(image, tf.float32)/256.0\n",
        "    image = tf.reshape(image, [28*28])\n",
        "    return image"
      ],
      "metadata": {
        "id": "PiuajYJ7fBJf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply this function to the dataset using .map and obtain a dataset of images.\n",
        "# The same kind of reading and decoding for is done using .zip for images and labels.\n",
        "\n",
        "def load_dataset(image_file, label_file):\n",
        "    imagedataset = tf.data.FixedLengthRecordDataset(image_file, 28*28, header_bytes=16)\n",
        "    imagedataset = imagedataset.map(read_image, num_parallel_calls=16)\n",
        "    labelsdataset = tf.data.FixedLengthRecordDataset(label_file, 1, header_bytes=8)\n",
        "    labelsdataset = labelsdataset.map(read_label, num_parallel_calls=16)\n",
        "    dataset = tf.data.Dataset.zip((imagedataset, labelsdataset))\n",
        "    return dataset "
      ],
      "metadata": {
        "id": "Yb9-x13mfJHn"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset\n",
        "# The tf.data.Dataset API has all the necessary utility functions for preparing datasets.\n",
        "# .cache caches the dataset in RAM\n",
        "# .shuffle shuffles it with a buffer of 5000 elements\n",
        "# .repeat loops the dataset\n",
        "# .batch pulls multiple images and labels together into a mini-batch\n",
        "# .prefetch can use the CPU to prepare the next batch while the current batch is being trained on the GPU\n",
        "# The validation dataset is prepared in a similar way.\n",
        "\n",
        "def get_training_dataset(image_file, label_file, batch_size):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache()  # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
        "    dataset = dataset.shuffle(5000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder is important on TPU, batch size must be fixed\n",
        "    dataset = dataset.prefetch(AUTO)  # fetch next batches while training on the current one (-1: autotune prefetch buffer size)\n",
        "    return dataset\n",
        "  \n",
        "def get_validation_dataset(image_file, label_file):\n",
        "    dataset = load_dataset(image_file, label_file)\n",
        "    dataset = dataset.cache() # this small dataset can be entirely cached in RAM, for TPU this is important to get good performance from such a small dataset\n",
        "    dataset = dataset.batch(10000, drop_remainder=True) # 10000 items in eval dataset, all in one batch\n",
        "    dataset = dataset.repeat() # Mandatory for Keras for now\n",
        "    return dataset\n",
        "\n",
        "# instantiate the datasets\n",
        "training_dataset = get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
        "validation_dataset = get_validation_dataset(validation_images_file, validation_labels_file)\n",
        "\n",
        "# For TPU, we will need a function that returns the dataset\n",
        "training_input_fn = lambda: get_training_dataset(training_images_file, training_labels_file, BATCH_SIZE)\n",
        "validation_input_fn = lambda: get_validation_dataset(validation_images_file, validation_labels_file)"
      ],
      "metadata": {
        "id": "g2CfB0VcfWyc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keras Model\n",
        "# The models will be straight sequences of layers so tf.keras.Sequential is used to create them. \n",
        "# There are 10 neurons because we are classifying handwritten digits into 10 classes.\n",
        "# A Keras model also needs to know the shape of its inputs, tf.keras.layers.Input can be used to define it.\n",
        "# Configuring the model is done in Keras using the model.compile function. \n",
        "# A classification model requires a cross-entropy loss function, called 'categorical_crossentropy' in Keras. \n",
        "# The model computes the 'accuracy' metric, which is the percentage of correctly classified images.\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "  [\n",
        "      tf.keras.layers.Reshape(input_shape=(28*28,), target_shape=(28, 28, 1)),\n",
        "      \n",
        "      tf.keras.layers.Conv2D(kernel_size=3, filters=12, use_bias=False, padding='same'),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Conv2D(kernel_size=6, filters=24, use_bias=False, padding='same', strides=2),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Conv2D(kernel_size=6, filters=32, use_bias=False, padding='same', strides=2),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Flatten(),\n",
        "      \n",
        "      tf.keras.layers.Dense(200, use_bias=False),\n",
        "      tf.keras.layers.BatchNormalization(center=True, scale=False),\n",
        "      tf.keras.layers.Activation('relu'),\n",
        "      \n",
        "      tf.keras.layers.Dropout(0.3),\n",
        "      tf.keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "\n",
        "# Handwritten digits are made of shapes and this shape information is lost when the pixels are flattened to make a single vector.\n",
        "# Convolutional networks can leverage the shape information.\n",
        "# Convolutional neural networks apply a series of learnable filters to the input image. \n",
        "# A convolutional layer is defined by the filter size, the number of filters applied and the stride. \n",
        "# The input and the output of a convolutional layer each have three dimensions (width, height, number of channels), starting with the input image (width, height, RGB channels). \n",
        "# To fix signs of overfitting a dropout layer is added.\n",
        "# Batch normalization helps neural networks converge and usually allows you to train faster by allowing the network to decide how much centering and re-scaling to apply at each neuron.\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Implementing a learning rate schedule that decays the learning rate exponentially. \n",
        "# lr decay function\n",
        "def lr_decay(epoch):\n",
        "  return 0.01 * math.pow(0.6, epoch)\n",
        "\n",
        "# lr schedule callback\n",
        "lr_decay_callback = tf.keras.callbacks.LearningRateScheduler(lr_decay, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DI-bI2tofr0t",
        "outputId": "e685e7c3-6b80-4e18-8d25-d0564fa71adc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 28, 28, 1)         0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 28, 28, 12)        108       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 28, 28, 12)       36        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 28, 28, 12)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 24)        10368     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 14, 14, 24)       72        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 14, 14, 24)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 32)          27648     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 7, 7, 32)         96        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 7, 7, 32)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 200)               313600    \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 200)              600       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 200)               0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 200)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 10)                2010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 354,538\n",
            "Trainable params: 354,002\n",
            "Non-trainable params: 536\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and validate the model\n",
        "# The training happens by calling model.fit and passing in both the training and validation datasets.\n",
        "\n",
        "steps_per_epoch = 60000//BATCH_SIZE  # 60,000 items in this dataset\n",
        "print(\"Steps per epoch: \", steps_per_epoch)\n",
        "\n",
        "history = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, validation_data=validation_dataset, validation_steps=1, callbacks=[lr_decay_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaFzeEvqgEs1",
        "outputId": "069ef386-081a-4d1a-98b7-8458c5f27e91"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch:  468\n",
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n",
            "Epoch 1/10\n",
            "468/468 [==============================] - 97s 204ms/step - loss: 0.1227 - accuracy: 0.9628 - val_loss: 0.1032 - val_accuracy: 0.9687 - lr: 0.0100\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 0.006.\n",
            "Epoch 2/10\n",
            "468/468 [==============================] - 93s 198ms/step - loss: 0.0415 - accuracy: 0.9869 - val_loss: 0.0451 - val_accuracy: 0.9866 - lr: 0.0060\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0036.\n",
            "Epoch 3/10\n",
            "468/468 [==============================] - 95s 202ms/step - loss: 0.0219 - accuracy: 0.9931 - val_loss: 0.0259 - val_accuracy: 0.9911 - lr: 0.0036\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.0021599999999999996.\n",
            "Epoch 4/10\n",
            "468/468 [==============================] - 92s 197ms/step - loss: 0.0143 - accuracy: 0.9955 - val_loss: 0.0236 - val_accuracy: 0.9923 - lr: 0.0022\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.001296.\n",
            "Epoch 5/10\n",
            "468/468 [==============================] - 94s 201ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0193 - val_accuracy: 0.9923 - lr: 0.0013\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.0007775999999999998.\n",
            "Epoch 6/10\n",
            "468/468 [==============================] - 93s 199ms/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0203 - val_accuracy: 0.9932 - lr: 7.7760e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0004665599999999999.\n",
            "Epoch 7/10\n",
            "468/468 [==============================] - 94s 200ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0190 - val_accuracy: 0.9928 - lr: 4.6656e-04\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.00027993599999999994.\n",
            "Epoch 8/10\n",
            "468/468 [==============================] - 93s 198ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.0188 - val_accuracy: 0.9929 - lr: 2.7994e-04\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.00016796159999999993.\n",
            "Epoch 9/10\n",
            "468/468 [==============================] - 94s 201ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0186 - val_accuracy: 0.9936 - lr: 1.6796e-04\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.00010077695999999997.\n",
            "Epoch 10/10\n",
            "468/468 [==============================] - 93s 198ms/step - loss: 0.0029 - accuracy: 0.9995 - val_loss: 0.0188 - val_accuracy: 0.9938 - lr: 1.0078e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The shape of the output tensor is [128, 10] because 128 images are processed and computing the argmax across the 10 probabilities returned for each image, thus axis=1.\n",
        "# This model recognises more than 99% of the digits."
      ],
      "metadata": {
        "id": "Pw4dzgfnh_o6"
      },
      "execution_count": 47,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}